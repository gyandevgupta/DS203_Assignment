{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "distilbert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzP6kNPGIqbg"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import sklearn.model_selection"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "BXQN16hseFj9",
        "outputId": "504ec6fc-865f-48b9-84da-7d74a07be6a1"
      },
      "source": [
        "url1 = \"https://raw.githubusercontent.com/gyandevgupta/DS203_Assignment/master/test-Assign8.csv\"\n",
        "url2 = \"https://raw.githubusercontent.com/gyandevgupta/DS203_Assignment/master/train-Assign8.csv\"\n",
        "df1 = pd.read_csv(url1, encoding='utf8')\n",
        "df2 = pd.read_csv(url2, encoding='utf8')\n",
        "df2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Reviews</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>When I first tuned in on this morning news, I ...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mere thoughts of \"Going Overboard\" (aka \"Babes...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Why does this movie fall WELL below standards?...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Wow and I thought that any Steven Segal movie ...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The story is seen before, but that does'n matt...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>Everyone plays their part pretty well in this ...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>It happened with Assault on Prescient 13 in 20...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>My God. This movie was awful. I can't complain...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>When I first popped in Happy Birthday to Me, I...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>So why does this show suck? Unfortunately, tha...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Reviews Sentiment\n",
              "0      When I first tuned in on this morning news, I ...       neg\n",
              "1      Mere thoughts of \"Going Overboard\" (aka \"Babes...       neg\n",
              "2      Why does this movie fall WELL below standards?...       neg\n",
              "3      Wow and I thought that any Steven Segal movie ...       neg\n",
              "4      The story is seen before, but that does'n matt...       neg\n",
              "...                                                  ...       ...\n",
              "24995  Everyone plays their part pretty well in this ...       pos\n",
              "24996  It happened with Assault on Prescient 13 in 20...       neg\n",
              "24997  My God. This movie was awful. I can't complain...       neg\n",
              "24998  When I first popped in Happy Birthday to Me, I...       neg\n",
              "24999  So why does this show suck? Unfortunately, tha...       neg\n",
              "\n",
              "[25000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "Vchz0e2-EBFv",
        "outputId": "30419d7d-a7d9-4eaa-bebe-4e4c24c39896"
      },
      "source": [
        "df = pd.concat([df1,df2], ignore_index=True)\n",
        "df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Reviews</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Who would have thought that a movie about a ma...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>After realizing what is going on around us ......</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I grew up watching the original Disney Cindere...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>David Mamet wrote the screenplay and made his ...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Admittedly, I didn't have high expectations of...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>Everyone plays their part pretty well in this ...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>It happened with Assault on Prescient 13 in 20...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>My God. This movie was awful. I can't complain...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>When I first popped in Happy Birthday to Me, I...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>So why does this show suck? Unfortunately, tha...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Reviews Sentiment\n",
              "0      Who would have thought that a movie about a ma...       pos\n",
              "1      After realizing what is going on around us ......       pos\n",
              "2      I grew up watching the original Disney Cindere...       neg\n",
              "3      David Mamet wrote the screenplay and made his ...       pos\n",
              "4      Admittedly, I didn't have high expectations of...       neg\n",
              "...                                                  ...       ...\n",
              "49995  Everyone plays their part pretty well in this ...       pos\n",
              "49996  It happened with Assault on Prescient 13 in 20...       neg\n",
              "49997  My God. This movie was awful. I can't complain...       neg\n",
              "49998  When I first popped in Happy Birthday to Me, I...       neg\n",
              "49999  So why does this show suck? Unfortunately, tha...       neg\n",
              "\n",
              "[50000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9oosu2WO5qS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "eca40594-cba6-426e-a986-da1869dedffa"
      },
      "source": [
        "print(\"Before PreProcessing : \\n\")\n",
        "print(df.iloc[1][\"Reviews\"])\n",
        "import re\n",
        "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
        "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "df[\"Reviews\"] = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in df[\"Reviews\"]]\n",
        "df[\"Reviews\"] = [REPLACE_WITH_SPACE.sub(\" \", line) for line in df[\"Reviews\"]]\n",
        "print(\"After PreProcessing: \\n\")\n",
        "df.iloc[1][\"Reviews\"]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before PreProcessing : \n",
            "\n",
            "After realizing what is going on around us ... in the news .. in our homes .. the whole new world .. I remembered this show and how obsessed I was watching it every week (in my town) ..<br /><br />I started looking for this series .. 3 days ago .. didn;t have luck till this moment .. and I was shocked when I read about it and about CBS ..<br /><br />People, I believe they stopped the show because it's talking about something way ahead of our understanding of the new world ... it was trying to deliver a hidden message about something terrifying ..<br /><br />The people who stopped it are the same who are controlling the world Now .. I remember in one of the episodes it was talking about the ONE dollar and the pyramid with the one eye ...\n",
            "After PreProcessing: \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'after realizing what is going on around us  in the news  in our homes  the whole new world  i remembered this show and how obsessed i was watching it every week in my town  i started looking for this series  3 days ago  didnt have luck till this moment  and i was shocked when i read about it and about cbs  people i believe they stopped the show because its talking about something way ahead of our understanding of the new world  it was trying to deliver a hidden message about something terrifying  the people who stopped it are the same who are controlling the world now  i remember in one of the episodes it was talking about the one dollar and the pyramid with the one eye '"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "P1WqsecXPfSI",
        "outputId": "2724a99e-6566-4739-b366-183a433cbb23"
      },
      "source": [
        "df.iloc[1][\"Reviews\"]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'after realizing what is going on around us  in the news  in our homes  the whole new world  i remembered this show and how obsessed i was watching it every week in my town  i started looking for this series  3 days ago  didnt have luck till this moment  and i was shocked when i read about it and about cbs  people i believe they stopped the show because its talking about something way ahead of our understanding of the new world  it was trying to deliver a hidden message about something terrifying  the people who stopped it are the same who are controlling the world now  i remember in one of the episodes it was talking about the one dollar and the pyramid with the one eye '"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub1EY-8uG94k"
      },
      "source": [
        "2. Split it into train and validation in the ratio 80:20\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rGS75AsFZr4",
        "outputId": "54288fbe-27b7-4fc5-859f-9a3f08f92e47"
      },
      "source": [
        "X = df.drop([\"Sentiment\"], axis=1)\n",
        "Y = df[\"Reviews\"]\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X, Y, test_size=0.2)\n",
        "# X_validate, X_test, y_validate, y_test = sklearn.model_selection.train_test_split(X_rem, y_rem, test_size=0.5)\n",
        "X_train.astype(str)\n",
        "print(X_train.dtype)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR9w0NE7G8FN"
      },
      "source": [
        "3. Install transformers library of huggingface\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdyg_gqcGuoI",
        "outputId": "e0ecb097-d110-4373-a9ce-447c6bd373d4"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIAuGU-P3Xbt"
      },
      "source": [
        "X_train_list = X_train.tolist()\n",
        "X_val_list = X_val.tolist()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eZjDL8gHBIS"
      },
      "source": [
        "4. From transformers import DistilBertTokenizerFast tokenizer of\n",
        "pretrained model “distilbert-base-uncased”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVT_T2wBHB6_"
      },
      "source": [
        "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "X_train_encoded = []\n",
        "X_val_encoded = []\n",
        "# tokens = tokenizer.basic_tokenizer.tokenize(text)\n",
        "# print(\"Tokens: \", tokens)\n",
        "# X_train_list = X_train.tolist()\n",
        "# X_val_list = X_val.tolist()\n",
        "# text = \"Replace me by any text you'd like.\"\n",
        "# X_train_list = X_train.tolist()\n",
        "# encoded_input_list = []\n",
        "# output_list = []\n",
        "\n",
        "# for i in range(0,40000):\n",
        "#   encoded_input = tokenizer(X_train_list[i], return_tensors='pt', max_length=512, truncation=True)\n",
        "#   encoded_input_list.append(encoded_input)\n",
        "#   output = model(**encoded_input) \n",
        "#   output_list.append(output)\n",
        "for i in range(0,40000):\n",
        "  encoded_input = tokenizer(X_train_list[i], truncation=True, padding = True, return_tensors = 'pt')\n",
        "  X_train_encoded.append(encoded_input)\n",
        "for i in range(0,10000):\n",
        "  encoded_input = tokenizer(X_val_list[i], truncation=True, padding = True, return_tensors = 'pt')\n",
        "  X_val_encoded.append(encoded_input)\n",
        "# X_train_encoded = tokenizer(X_train_list, truncation=True, padding = True)\n",
        "# X_val_encoded = tokenizer(X_val_list, truncation=True, padding = True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4u09fEOH9NR"
      },
      "source": [
        "class MyDistilbert(Dataset):\n",
        "\n",
        "  def __init__(self,data,labels) :\n",
        "    self.data = data\n",
        "    self.labels = labels\n",
        "\n",
        "  def __getitem__(self,index) :\n",
        "    X = self.data[index]\n",
        "    Y = self.labels[index]\n",
        "    return X,Y\n",
        "    # item = {key: torch.tensor(val[index]) for key, val in self.data.items()}\n",
        "    # item['labels'] = torch.tensor(self.labels[index])\n",
        "    # return item\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laazISNpBPDG"
      },
      "source": [
        "train_dataset = MyDistilbert(X_train_encoded,y_train)\n",
        "val_dataset = MyDistilbert(X_val_encoded,y_val)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0jQRMNCnlpo"
      },
      "source": [
        "train_load = DataLoader(train_dataset, drop_last=False ,batch_size=128, shuffle=True , collate_fn=lambda x: x)\n",
        "val_load = DataLoader(val_dataset, drop_last=False, batch_size=128, shuffle=True, collate_fn=lambda x: x)  "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBf0UI7Q6AP7"
      },
      "source": [
        "class Network(torch.nn.Module):\n",
        "  def __init__(self,hidden_layer_size = 4):\n",
        "    super(Network,self).__init__()\n",
        "    self.layer1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "    self.dropout = torch.nn.Dropout(p=0.25)\n",
        "    self.linear1 = torch.nn.Linear(2, hidden_layer_size)\n",
        "    self.linear2 = torch.nn.Linear(hidden_layer_size, 1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x = self.dropout(x)\n",
        "    x = torch.nn.functional.relu(self.linear1(x)) \n",
        "    out = torch.sigmoid(self.linear2(x)) \n",
        "    return out"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r1zozZ3Uf4U"
      },
      "source": [
        "def MyModel(hidden_layer_size=4, learning_rate=0.003, num_epoch = 25):\n",
        "  model = Network(hidden_layer_size=4)\n",
        "  loss_function = torch.nn.BCELoss(reduction='sum') # we would set mean for the loss\n",
        "  optimizer = torch.optim.SGD(model.parameters(), learning_rate) \n",
        "\n",
        "  train_loss = []\n",
        "  val_loss = []\n",
        "  train_accuracy = []\n",
        "  val_accuracy = [] \n",
        "\n",
        "  for ind in range(num_epoch):\n",
        "    tot_train_loss = 0\n",
        "    tot_train_accuracy = 0\n",
        "    tot_val_loss = 0\n",
        "    tot_val_accuracy = 0\n",
        "\n",
        "    for batch in train_load:\n",
        "      batch_x = batch[0]\n",
        "      batch_y = batch[1]\n",
        "      model.train()\n",
        "      y_pred = model(batch_x)\n",
        "      loss = loss_function(np.array(y_pred),np.array(batch_y))/16 \n",
        "      tot_train_loss+=16*loss\n",
        "      tot_train_accuracy+= ((y_pred>0.5)== batch_y).sum()\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    train_accuracy.append(tot_train_accuracy.detach().numpy()/400)\n",
        "    train_loss.append(tot_train_loss.detach().numpy()/40000)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "\n",
        "      for batch in val_load:\n",
        "        batch_x = batch[0]\n",
        "        batch_y = batch[1]\n",
        "        y_pred_val = model(batch_x.float())\n",
        "        loss = loss_function(y_pred_val.float(),batch_y.float())\n",
        "        tot_val_loss+=loss\n",
        "        tot_val_accuracy+= ((y_pred_val>0.5)== batch_y).sum()\n",
        "\n",
        "      val_accuracy.append(tot_val_accuracy.detach().numpy()/100)\n",
        "      val_loss.append(tot_val_loss.detach().numpy()/10000)\n",
        "  return model,train_accuracy,train_loss,val_accuracy,val_loss"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "XJngT-fen3m7",
        "outputId": "700098d2-64b5-4e11-e53d-0bce91ac894a"
      },
      "source": [
        "hidden_layer_size = 4 \n",
        "learning_rate = 0.003\n",
        "num_epoch = 25\n",
        "mod,train_accuracy,train_loss,val_accuracy,val_loss = MyModel(hidden_layer_size,learning_rate,num_epoch)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-9cba0da145b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.003\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-71ebce775fc8>\u001b[0m in \u001b[0;36mMyModel\u001b[0;34m(hidden_layer_size, learning_rate, num_epoch)\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mtot_train_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-b7ab0f61e419>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You cannot specify both input_ids and inputs_embeds at the same time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYhDVynK2ZH1"
      },
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "validation_loss, = plt.plot(val_loss, label = 'Validation Loss')\n",
        "training_loss, = plt.plot(train_loss, label = 'Training Loss')\n",
        "plt.title('Training and Validation loss vs epoch')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcHal8FG2ZGT"
      },
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "validation_accuracy, = plt.plot(val_accuracy, label = 'Validation Accuracy')\n",
        "training_accuracy, = plt.plot(train_accuracy, label = 'Training Accuracy')\n",
        "plt.title('Training and Validation accuracy vs epoch')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}